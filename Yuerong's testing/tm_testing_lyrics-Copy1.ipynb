{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove stopwords\n",
    "## we will replace this word list with our own list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Stopwords\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Year</th>\n",
       "      <th>CombinedLyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>i can feel the magic floating in the air bein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>im desperate for changing starving for truth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2002</td>\n",
       "      <td>never made it as a wise man i couldnt cut it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>go go go go go go go shawty its your birthday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2004</td>\n",
       "      <td>peace up atown down yeah ok lil jonyeah yeah ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2005</td>\n",
       "      <td>ooh ooh sweet love yeahi didnt mean it when i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>where is the moment when we needed the most y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2007</td>\n",
       "      <td>to the left to the left everything you own in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2008</td>\n",
       "      <td>shawty had them apple bottom jeans jeans boot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2009</td>\n",
       "      <td>gotta getget gotta getget gotta getget gotta g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2010</td>\n",
       "      <td>wake up in the morning feeling like p diddy h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2011</td>\n",
       "      <td>theres a fire starting in my heart reaching a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2012</td>\n",
       "      <td>gotye now and then i think of when we were to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>2013</td>\n",
       "      <td>hook wanz im gonna pop some tags only got twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>2014</td>\n",
       "      <td>it might seem crazy what im about to say suns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>2015</td>\n",
       "      <td>this hit that ice cold michelle pfeiffer that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Year                                     CombinedLyrics\n",
       "0            0  2000   i can feel the magic floating in the air bein...\n",
       "1            1  2001   im desperate for changing starving for truth ...\n",
       "2            2  2002   never made it as a wise man i couldnt cut it ...\n",
       "3            3  2003   go go go go go go go shawty its your birthday...\n",
       "4            4  2004   peace up atown down yeah ok lil jonyeah yeah ...\n",
       "5            5  2005   ooh ooh sweet love yeahi didnt mean it when i...\n",
       "6            6  2006   where is the moment when we needed the most y...\n",
       "7            7  2007  to the left to the left everything you own in ...\n",
       "8            8  2008   shawty had them apple bottom jeans jeans boot...\n",
       "9            9  2009  gotta getget gotta getget gotta getget gotta g...\n",
       "10          10  2010   wake up in the morning feeling like p diddy h...\n",
       "11          11  2011   theres a fire starting in my heart reaching a...\n",
       "12          12  2012   gotye now and then i think of when we were to...\n",
       "13          13  2013   hook wanz im gonna pop some tags only got twe...\n",
       "14          14  2014   it might seem crazy what im about to say suns...\n",
       "15          15  2015   this hit that ice cold michelle pfeiffer that..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_csv('lyricsByYear.csv',sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.CombinedLyrics.values.tolist()\n",
    "#pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize words and Clean-up text\n",
    "- tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
    "- Gensim’s simple_preprocess() is great for this. Additionally I have set deacc=True to remove the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Bigram and Trigram Models\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.Some examples in our example are: ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.\n",
    "- Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. \n",
    "- The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "#print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Stopwords, Make Bigrams and Lemmatize\n",
    "- define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "#print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Dictionary and Corpus needed for Topic Modeling\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "#print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'able'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the best k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a human-readable form of the corpus itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('able', 1),\n",
       "  ('aboutbut', 1),\n",
       "  ('absolutely', 6),\n",
       "  ('accept', 2),\n",
       "  ('account', 1),\n",
       "  ('act', 30),\n",
       "  ('actor', 1),\n",
       "  ('add', 2),\n",
       "  ('admit', 2),\n",
       "  ('affair', 1),\n",
       "  ('afford', 1),\n",
       "  ('afraid', 3),\n",
       "  ('againand', 2),\n",
       "  ('againcause', 1),\n",
       "  ('againhope', 1),\n",
       "  ('againhow', 1),\n",
       "  ('againim', 1),\n",
       "  ('againit', 2),\n",
       "  ('againsome', 1),\n",
       "  ('againthe', 2),\n",
       "  ('againwhen', 1),\n",
       "  ('ago', 4),\n",
       "  ('agree', 2),\n",
       "  ('agreement', 1),\n",
       "  ('ahead', 1),\n",
       "  ('ahora_vengo', 8),\n",
       "  ('aight', 1),\n",
       "  ('air', 2),\n",
       "  ('airplane', 2),\n",
       "  ('album', 1),\n",
       "  ('alittle', 1),\n",
       "  ('alive', 14),\n",
       "  ('allow', 1),\n",
       "  ('allwhen', 1),\n",
       "  ('almost', 1),\n",
       "  ('alone', 44),\n",
       "  ('alonebut', 1),\n",
       "  ('alonelet', 4),\n",
       "  ('alonewhen', 1),\n",
       "  ('aloneyoure', 1),\n",
       "  ('already', 6),\n",
       "  ('alright', 7),\n",
       "  ('alrightif', 1),\n",
       "  ('altar', 5),\n",
       "  ('always', 53),\n",
       "  ('amazed', 3),\n",
       "  ('amazing', 3),\n",
       "  ('amistad_couldnt', 4),\n",
       "  ('amount', 4),\n",
       "  ('anew', 1),\n",
       "  ('angel', 18),\n",
       "  ('angelsall', 1),\n",
       "  ('anger', 3),\n",
       "  ('animal', 1),\n",
       "  ('anotheri', 1),\n",
       "  ('anotherwhen', 1),\n",
       "  ('answer', 2),\n",
       "  ('antelope', 1),\n",
       "  ('antibody', 1),\n",
       "  ('antidote', 1),\n",
       "  ('anybodyit', 1),\n",
       "  ('anyway', 1),\n",
       "  ('apart', 9),\n",
       "  ('apartment', 1),\n",
       "  ('appeal', 1),\n",
       "  ('appear', 1),\n",
       "  ('appreciate', 2),\n",
       "  ('arei', 1),\n",
       "  ('areshow', 1),\n",
       "  ('argue', 1),\n",
       "  ('arm', 22),\n",
       "  ('arms_wide', 13),\n",
       "  ('around', 22),\n",
       "  ('ashtrays', 1),\n",
       "  ('ask', 22),\n",
       "  ('asleep', 1),\n",
       "  ('ass', 28),\n",
       "  ('assassin', 1),\n",
       "  ('assattention', 1),\n",
       "  ('assshake', 3),\n",
       "  ('assume', 3),\n",
       "  ('attention', 4),\n",
       "  ('attitude', 1),\n",
       "  ('attractive', 1),\n",
       "  ('awake', 2),\n",
       "  ('away', 35),\n",
       "  ('awayall', 1),\n",
       "  ('awayoh', 6),\n",
       "  ('awe', 1),\n",
       "  ('awkward', 1),\n",
       "  ('babe', 2),\n",
       "  ('babeevery', 1),\n",
       "  ('babetell', 1),\n",
       "  ('baby', 257),\n",
       "  ('babygive', 1),\n",
       "  ('babyill', 1),\n",
       "  ('babymeet', 1),\n",
       "  ('babymy', 1),\n",
       "  ('babyoh', 2),\n",
       "  ('babyoop', 1),\n",
       "  ('babythere', 1),\n",
       "  ('back', 80),\n",
       "  ('backin', 1),\n",
       "  ('backseat', 2),\n",
       "  ('backthere', 1),\n",
       "  ('backwhatcha', 7),\n",
       "  ('bad', 20),\n",
       "  ('bag', 6),\n",
       "  ('bail', 1),\n",
       "  ('ball', 2),\n",
       "  ('balla', 11),\n",
       "  ('baller', 1),\n",
       "  ('band', 1),\n",
       "  ('bank', 1),\n",
       "  ('bar', 1),\n",
       "  ('bare', 2),\n",
       "  ('bark', 1),\n",
       "  ('basement', 1),\n",
       "  ('basically', 2),\n",
       "  ('basis', 1),\n",
       "  ('be', 652),\n",
       "  ('bead', 1),\n",
       "  ('bear', 2),\n",
       "  ('beat', 26),\n",
       "  ('beatin', 1),\n",
       "  ('beautiful', 5),\n",
       "  ('beautifulsmoke', 1),\n",
       "  ('beauty', 1),\n",
       "  ('become', 4),\n",
       "  ('bed', 4),\n",
       "  ('bedroom', 2),\n",
       "  ('beef', 1),\n",
       "  ('beemer', 1),\n",
       "  ('beep', 2),\n",
       "  ('beg', 2),\n",
       "  ('begged', 5),\n",
       "  ('begin', 2),\n",
       "  ('behind', 1),\n",
       "  ('bein', 5),\n",
       "  ('believe', 65),\n",
       "  ('believei', 2),\n",
       "  ('belong', 6),\n",
       "  ('belongscause', 1),\n",
       "  ('belt', 1),\n",
       "  ('bend', 9),\n",
       "  ('bended_knee', 9),\n",
       "  ('bent', 1),\n",
       "  ('bentif', 1),\n",
       "  ('bentley', 2),\n",
       "  ('bentstart', 1),\n",
       "  ('bes', 2),\n",
       "  ('beshe', 1),\n",
       "  ('beside', 1),\n",
       "  ('best', 7),\n",
       "  ('besweet', 2),\n",
       "  ('bet', 1),\n",
       "  ('betcha', 1),\n",
       "  ('bethink', 1),\n",
       "  ('better', 18),\n",
       "  ('bewhere', 1),\n",
       "  ('big', 32),\n",
       "  ('bill', 3),\n",
       "  ('billy', 1),\n",
       "  ('bind', 3),\n",
       "  ('bionic', 1),\n",
       "  ('birthday', 1),\n",
       "  ('bit', 2),\n",
       "  ('bitch', 8),\n",
       "  ('bite', 1),\n",
       "  ('bitter', 1),\n",
       "  ('blad', 6),\n",
       "  ('blame', 2),\n",
       "  ('blanket', 1),\n",
       "  ('blast', 1),\n",
       "  ('bleed', 2),\n",
       "  ('blessing', 1),\n",
       "  ('blew', 1),\n",
       "  ('blind', 6),\n",
       "  ('bling_bling', 3),\n",
       "  ('bloody', 1),\n",
       "  ('blow', 8),\n",
       "  ('blowin', 1),\n",
       "  ('blowsand', 1),\n",
       "  ('blowsoh', 1),\n",
       "  ('blowsyou', 3),\n",
       "  ('blue', 20),\n",
       "  ('blunt', 1),\n",
       "  ('bnigga', 1),\n",
       "  ('boat', 1),\n",
       "  ('body', 18),\n",
       "  ('bold', 1),\n",
       "  ('bomb', 1),\n",
       "  ('bone', 2),\n",
       "  ('bonnet', 1),\n",
       "  ('boo', 5),\n",
       "  ('boogiewoogie', 1),\n",
       "  ('book', 1),\n",
       "  ('bootie', 3),\n",
       "  ('booty', 14),\n",
       "  ('bottom', 19),\n",
       "  ('bounce', 7),\n",
       "  ('bounce_bounce', 11),\n",
       "  ('bourgeoisie', 3),\n",
       "  ('bout', 18),\n",
       "  ('bow', 3),\n",
       "  ('boy', 34),\n",
       "  ('boyfriend', 1),\n",
       "  ('boyz', 6),\n",
       "  ('boyzhot', 1),\n",
       "  ('bracelet', 1),\n",
       "  ('brag', 1),\n",
       "  ('braid', 3),\n",
       "  ('brain', 2),\n",
       "  ('brat', 2),\n",
       "  ('break', 36),\n",
       "  ('breakin', 1),\n",
       "  ('breath', 9),\n",
       "  ('breathe', 11),\n",
       "  ('breathecaught', 1),\n",
       "  ('breathei', 1),\n",
       "  ('breathein', 1),\n",
       "  ('breathin', 1),\n",
       "  ('briget', 1),\n",
       "  ('bright', 2),\n",
       "  ('bring', 29),\n",
       "  ('britney', 1),\n",
       "  ('broad', 1),\n",
       "  ('broken', 8),\n",
       "  ('broker', 1),\n",
       "  ('brother', 1),\n",
       "  ('brown', 1),\n",
       "  ('bubble', 1),\n",
       "  ('buck', 3),\n",
       "  ('buggin', 1),\n",
       "  ('build', 1),\n",
       "  ('bull', 1),\n",
       "  ('bullet', 1),\n",
       "  ('bump', 3),\n",
       "  ('bunch', 1),\n",
       "  ('burn', 15),\n",
       "  ('bury', 1),\n",
       "  ('bus', 1),\n",
       "  ('business', 1),\n",
       "  ('bust', 1),\n",
       "  ('busy', 1),\n",
       "  ('but', 1),\n",
       "  ('butt_butt', 2),\n",
       "  ('buttnake', 1),\n",
       "  ('buy', 15),\n",
       "  ('buyin', 1),\n",
       "  ('cabbage', 1),\n",
       "  ('cajun', 1),\n",
       "  ('call', 28),\n",
       "  ('callhow', 1),\n",
       "  ('campin', 1),\n",
       "  ('can', 154),\n",
       "  ('candidate', 1),\n",
       "  ('candy', 2),\n",
       "  ('canhe', 4),\n",
       "  ('cani', 1),\n",
       "  ('canim', 1),\n",
       "  ('cannibal', 1),\n",
       "  ('cantaloupe', 1),\n",
       "  ('cantuntil', 2),\n",
       "  ('car', 17),\n",
       "  ('care', 9),\n",
       "  ('caressay', 1),\n",
       "  ('carry', 11),\n",
       "  ('cash', 2),\n",
       "  ('cat', 3),\n",
       "  ('catch', 13),\n",
       "  ('cause', 60),\n",
       "  ('causei', 1),\n",
       "  ('ccumulate', 1),\n",
       "  ('celly', 1),\n",
       "  ('cemetery', 1),\n",
       "  ('century', 1),\n",
       "  ('chain', 13),\n",
       "  ('chair', 1),\n",
       "  ('chance', 8),\n",
       "  ('chancewhat', 1),\n",
       "  ('change', 22),\n",
       "  ('channel', 1),\n",
       "  ('character', 1),\n",
       "  ('charlie', 2),\n",
       "  ('chase', 3),\n",
       "  ('chased', 1),\n",
       "  ('cheap', 2),\n",
       "  ('cheat', 1),\n",
       "  ('cheb', 1),\n",
       "  ('check', 6),\n",
       "  ('checkin', 1),\n",
       "  ('cheese', 1),\n",
       "  ('chemistry', 1),\n",
       "  ('cherish', 2),\n",
       "  ('chestyall', 1),\n",
       "  ('chick', 2),\n",
       "  ('chicken', 1),\n",
       "  ('child', 3),\n",
       "  ('childrenand', 1),\n",
       "  ('chill', 1),\n",
       "  ('ching_che', 5),\n",
       "  ('choice', 4),\n",
       "  ('choke', 5),\n",
       "  ('choose', 2),\n",
       "  ('choosenow', 1),\n",
       "  ('chooseyour', 1),\n",
       "  ('chrome', 2),\n",
       "  ('chronice', 1),\n",
       "  ('church', 1),\n",
       "  ('circle', 1),\n",
       "  ('city', 6),\n",
       "  ('claim', 1),\n",
       "  ('clap', 1),\n",
       "  ('clap_clap', 4),\n",
       "  ('classic', 1),\n",
       "  ('clean', 1),\n",
       "  ('clear', 12),\n",
       "  ('click', 1),\n",
       "  ('climb', 1),\n",
       "  ('clique', 1),\n",
       "  ('clock', 1),\n",
       "  ('close', 21),\n",
       "  ('closely', 1),\n",
       "  ('closer', 9),\n",
       "  ('clothe', 7),\n",
       "  ('cloud', 1),\n",
       "  ('club', 23),\n",
       "  ('coat', 1),\n",
       "  ('cock', 9),\n",
       "  ('cocoa', 9),\n",
       "  ('coffee', 1),\n",
       "  ('cold', 5),\n",
       "  ('collar', 1),\n",
       "  ('color', 3),\n",
       "  ('come', 123),\n",
       "  ('comes_naturally', 1),\n",
       "  ('cometake', 1),\n",
       "  ('comfort', 3),\n",
       "  ('comin', 1),\n",
       "  ('commit', 1),\n",
       "  ('company', 1),\n",
       "  ('complete', 9),\n",
       "  ('completely', 2),\n",
       "  ('completion', 1),\n",
       "  ('complicated', 2),\n",
       "  ('compromise', 1),\n",
       "  ('concern', 1),\n",
       "  ('confess', 1),\n",
       "  ('confession', 1),\n",
       "  ('confidence', 1),\n",
       "  ('connect', 3),\n",
       "  ('conquer', 1),\n",
       "  ('constant', 2),\n",
       "  ('content', 1),\n",
       "  ('control', 3),\n",
       "  ('conversation', 1),\n",
       "  ('cook', 1),\n",
       "  ('cool', 11),\n",
       "  ('cop', 2),\n",
       "  ('corner', 3),\n",
       "  ('corvette', 2),\n",
       "  ('cost', 2),\n",
       "  ('could', 16),\n",
       "  ('count', 6),\n",
       "  ('counter', 1),\n",
       "  ('coupe', 1),\n",
       "  ('couple', 6),\n",
       "  ('courage', 1),\n",
       "  ('cover', 3),\n",
       "  ('coward', 2),\n",
       "  ('cowboy', 3),\n",
       "  ('coz', 3),\n",
       "  ('crack', 1),\n",
       "  ('crash', 18),\n",
       "  ('craze', 1),\n",
       "  ('crazy', 27),\n",
       "  ('crazymusicmusic', 1),\n",
       "  ('create', 1),\n",
       "  ('creep', 7),\n",
       "  ('crescent', 1),\n",
       "  ('crew', 1),\n",
       "  ('crib', 3),\n",
       "  ('crip', 1),\n",
       "  ('critic', 1),\n",
       "  ('critical', 2),\n",
       "  ('cross', 6),\n",
       "  ('crowd', 4),\n",
       "  ('crucial', 2),\n",
       "  ('cruel', 1),\n",
       "  ('cruise', 4),\n",
       "  ('crumb', 1),\n",
       "  ('crumble', 6),\n",
       "  ('crunk', 6),\n",
       "  ('crush', 1),\n",
       "  ('cry', 21),\n",
       "  ('cube', 1),\n",
       "  ('cuddle', 1),\n",
       "  ('cup', 1),\n",
       "  ('curious', 1),\n",
       "  ('curse', 1),\n",
       "  ('curve', 1),\n",
       "  ('cuss', 1),\n",
       "  ('cut', 5),\n",
       "  ('cute', 4),\n",
       "  ('d', 1),\n",
       "  ('da_ba', 25),\n",
       "  ('daai', 1),\n",
       "  ('daaim', 3),\n",
       "  ('daainside', 1),\n",
       "  ('dad', 5),\n",
       "  ('daily', 1),\n",
       "  ('damn', 2),\n",
       "  ('dance', 55),\n",
       "  ('dancedancei', 1),\n",
       "  ('dancei_hope', 3),\n",
       "  ('dap', 1),\n",
       "  ('dark', 11),\n",
       "  ('darkchild', 1),\n",
       "  ('darkness', 2),\n",
       "  ('date', 1),\n",
       "  ('daughter', 1),\n",
       "  ('day', 62),\n",
       "  ('daycause', 1),\n",
       "  ('daylet', 1),\n",
       "  ('dead', 7),\n",
       "  ('deal', 4),\n",
       "  ('dealer', 1),\n",
       "  ('deceive', 1),\n",
       "  ('decision', 1),\n",
       "  ('decorate', 1),\n",
       "  ('deep', 4),\n",
       "  ('deeper', 1),\n",
       "  ('def', 1),\n",
       "  ('deliver', 1),\n",
       "  ('demand', 1),\n",
       "  ('deniedim', 1),\n",
       "  ('deny', 3),\n",
       "  ('denyi', 1),\n",
       "  ('depend', 8),\n",
       "  ('describe', 1),\n",
       "  ('desert', 5),\n",
       "  ('deserve', 2),\n",
       "  ('designer', 1),\n",
       "  ('desire', 2),\n",
       "  ('desperate', 1),\n",
       "  ('destine', 1),\n",
       "  ('destinyoh', 1),\n",
       "  ('destroy', 1),\n",
       "  ('destruction', 1),\n",
       "  ('devilish', 2),\n",
       "  ('devilishshe', 1),\n",
       "  ('devulveme', 2),\n",
       "  ('diamond', 2),\n",
       "  ('diamond_ring', 6),\n",
       "  ('die', 7),\n",
       "  ('dieyou', 1),\n",
       "  ('difference', 2),\n",
       "  ('different', 1),\n",
       "  ('dig', 3),\n",
       "  ('digging', 1),\n",
       "  ('digit', 1),\n",
       "  ('dim', 2),\n",
       "  ('dime', 1),\n",
       "  ('dinosaur', 1),\n",
       "  ('dip', 2),\n",
       "  ('dirt', 2),\n",
       "  ('dirt_road', 1),\n",
       "  ('disappear', 1),\n",
       "  ('discover', 1),\n",
       "  ('disease', 1),\n",
       "  ('dismiss', 1),\n",
       "  ('distance', 1),\n",
       "  ('divorce', 2),\n",
       "  ('dizzy', 1),\n",
       "  ('dj', 2),\n",
       "  ('do', 18),\n",
       "  ('dobitche', 1),\n",
       "  ('doctor', 1),\n",
       "  ('docuz', 3),\n",
       "  ('dodeedodee', 2),\n",
       "  ('dodoubleg', 1),\n",
       "  ('doesnt_matter', 5),\n",
       "  ('dog', 4),\n",
       "  ('doive', 2),\n",
       "  ('dollar', 4),\n",
       "  ('dollare', 1),\n",
       "  ('door', 5),\n",
       "  ('dope', 7),\n",
       "  ('dot', 3),\n",
       "  ('double', 2),\n",
       "  ('doubt', 6),\n",
       "  ('dough', 8),\n",
       "  ('dountil', 3),\n",
       "  ('downi', 1),\n",
       "  ('downyou', 1),\n",
       "  ('drag', 1),\n",
       "  ('draw', 1),\n",
       "  ('dre', 2),\n",
       "  ('dream', 57),\n",
       "  ('dreambut', 1),\n",
       "  ('dreamed', 4),\n",
       "  ('drenowaday', 4),\n",
       "  ('dreso', 1),\n",
       "  ('dress', 15),\n",
       "  ('drink', 4),\n",
       "  ('drip', 1),\n",
       "  ('drive', 10),\n",
       "  ('drop', 8),\n",
       "  ('drown', 5),\n",
       "  ('drug', 1),\n",
       "  ('drunk', 1),\n",
       "  ('dry', 1),\n",
       "  ('dub', 1),\n",
       "  ('duck', 1),\n",
       "  ('due', 1),\n",
       "  ('duh', 2),\n",
       "  ('dumb', 2),\n",
       "  ('dump', 11),\n",
       "  ('dust', 25),\n",
       "  ('ear', 1),\n",
       "  ('earth', 2),\n",
       "  ('ease', 3),\n",
       "  ('easily', 1),\n",
       "  ('east_coast', 1),\n",
       "  ('easy', 7),\n",
       "  ('eat', 3),\n",
       "  ('eazyes', 1),\n",
       "  ('echo', 1),\n",
       "  ('ecstasy', 1),\n",
       "  ('effervescent', 1),\n",
       "  ('elope', 1),\n",
       "  ('else', 19),\n",
       "  ('elseand', 1),\n",
       "  ('elsei', 1),\n",
       "  ('elseit', 1),\n",
       "  ('elsemaybe', 1),\n",
       "  ('embrace', 5),\n",
       "  ('emotion', 7),\n",
       "  ('emotional', 1),\n",
       "  ('empty', 5),\n",
       "  ('end', 25),\n",
       "  ('endfrom', 1),\n",
       "  ('endless', 2),\n",
       "  ('energy', 1),\n",
       "  ('energybaby', 1),\n",
       "  ('energyoh', 2),\n",
       "  ('engine', 1),\n",
       "  ('enough', 24),\n",
       "  ('ensure', 1),\n",
       "  ('entertainin', 1),\n",
       "  ('episode', 1),\n",
       "  ('erybody', 2),\n",
       "  ('escape', 3),\n",
       "  ('est', 1),\n",
       "  ('eternally', 2),\n",
       "  ('eternity', 6),\n",
       "  ('even', 38),\n",
       "  ('ever', 35),\n",
       "  ('everi', 1),\n",
       "  ('everturn', 1),\n",
       "  ('everyday', 4),\n",
       "  ('everydaymusic', 1),\n",
       "  ('everythingwith', 1),\n",
       "  ('everytime', 2),\n",
       "  ('everywhere', 1),\n",
       "  ('eviction', 1),\n",
       "  ('evident', 1),\n",
       "  ('exactly', 6),\n",
       "  ('exercise', 1),\n",
       "  ('exist', 1),\n",
       "  ('existence', 1),\n",
       "  ('expect', 1),\n",
       "  ('expensive', 4),\n",
       "  ('experience', 1),\n",
       "  ('explain', 1),\n",
       "  ('eye', 43),\n",
       "  ('eyesmaria', 1),\n",
       "  ('eyesshook', 1),\n",
       "  ('fabulous', 1),\n",
       "  ('face', 11),\n",
       "  ('factor', 1),\n",
       "  ('fade', 7),\n",
       "  ('faded', 3),\n",
       "  ('fail', 1),\n",
       "  ('faint', 1),\n",
       "  ('fair', 1),\n",
       "  ('faith', 9),\n",
       "  ('fake', 1),\n",
       "  ('fall', 54),\n",
       "  ('fallcheb', 1),\n",
       "  ('false', 1),\n",
       "  ('fame', 3),\n",
       "  ('family', 3),\n",
       "  ('familymeet', 1),\n",
       "  ('familys', 1),\n",
       "  ('fan', 1),\n",
       "  ('fancy', 2),\n",
       "  ('fancy_car', 5),\n",
       "  ('far', 6),\n",
       "  ('fare', 1),\n",
       "  ('farewell', 1),\n",
       "  ('fast', 3),\n",
       "  ('fate', 3),\n",
       "  ('fault', 1),\n",
       "  ('favor', 1),\n",
       "  ('faze', 3),\n",
       "  ('fear', 4),\n",
       "  ('feed', 1),\n",
       "  ('feel', 155),\n",
       "  ('feeling', 8),\n",
       "  ('feelings_start', 10),\n",
       "  ('fella', 11),\n",
       "  ('feminist', 1),\n",
       "  ('fide', 1),\n",
       "  ('field', 1),\n",
       "  ('fiende', 1),\n",
       "  ('fiendini', 3),\n",
       "  ('fifteenth', 1),\n",
       "  ('fight', 5),\n",
       "  ('figure', 3),\n",
       "  ('figured', 1),\n",
       "  ('fill', 1),\n",
       "  ('finally', 12),\n",
       "  ('find', 45),\n",
       "  ('fine', 4),\n",
       "  ('finer_thing', 3),\n",
       "  ('finger', 2),\n",
       "  ('finish', 1),\n",
       "  ('fire', 10),\n",
       "  ('firm', 1),\n",
       "  ('firmyou', 1),\n",
       "  ('first', 32),\n",
       "  ('firstwhatcha', 1),\n",
       "  ('fist', 1),\n",
       "  ('fit', 1),\n",
       "  ('fittin', 1),\n",
       "  ('fix', 1),\n",
       "  ('flaunt', 1),\n",
       "  ('fling', 3),\n",
       "  ('flirt', 1),\n",
       "  ('float', 4),\n",
       "  ('floor', 10),\n",
       "  ('flop', 1),\n",
       "  ('flow', 2),\n",
       "  ('flower', 3),\n",
       "  ('fly', 23),\n",
       "  ('flyin', 1),\n",
       "  ('fo', 1),\n",
       "  ('folk', 1),\n",
       "  ('fool', 9),\n",
       "  ('foot', 11),\n",
       "  ('forbid', 1),\n",
       "  ('fore', 3),\n",
       "  ('foreplay', 5),\n",
       "  ('forever', 20),\n",
       "  ('foreverim', 1),\n",
       "  ('foreverin', 1),\n",
       "  ('forevermy', 1),\n",
       "  ('forget', 16),\n",
       "  ('forhere', 1),\n",
       "  ('fortune', 2),\n",
       "  ('forward', 1),\n",
       "  ('frankly', 1),\n",
       "  ('freak', 4),\n",
       "  ('free', 16),\n",
       "  ('freedom', 2),\n",
       "  ('freely', 1),\n",
       "  ('freeze', 1),\n",
       "  ('fresh', 1),\n",
       "  ('friend', 51),\n",
       "  ('friendi', 1),\n",
       "  ('friendyoure', 1),\n",
       "  ('front', 10),\n",
       "  ('fuck', 22),\n",
       "  ('fuckin', 4),\n",
       "  ('full', 18),\n",
       "  ('full_grown', 1),\n",
       "  ('fully', 1),\n",
       "  ('fun', 4),\n",
       "  ('funkdafie', 1),\n",
       "  ('funny', 2),\n",
       "  ('funyour', 1),\n",
       "  ('furniture', 1),\n",
       "  ('future', 1),\n",
       "  ('gable', 5),\n",
       "  ('game', 30),\n",
       "  ('gamewhat', 1),\n",
       "  ('gangstas', 1),\n",
       "  ('garage', 1),\n",
       "  ('gas', 1),\n",
       "  ('gat', 1),\n",
       "  ('gate', 3),\n",
       "  ('gaze', 2),\n",
       "  ('gegegeyeah', 1),\n",
       "  ('gem', 1),\n",
       "  ('gent', 1),\n",
       "  ('german', 1),\n",
       "  ('get', 274),\n",
       "  ('gettin', 1),\n",
       "  ('ghetto', 1),\n",
       "  ('gift', 1),\n",
       "  ('girl', 186),\n",
       "  ('girlall', 1),\n",
       "  ('girlfriend', 2),\n",
       "  ('girlhe', 4),\n",
       "  ('girlsforgotten', 1),\n",
       "  ('give', 77),\n",
       "  ('givesthink', 1),\n",
       "  ('glad', 3),\n",
       "  ('gladly', 1),\n",
       "  ('glamour', 1),\n",
       "  ('glance', 1),\n",
       "  ('glock', 2),\n",
       "  ('go', 361),\n",
       "  ('goaround', 1),\n",
       "  ('gocause', 4),\n",
       "  ('goim', 1),\n",
       "  ('gold', 1),\n",
       "  ('golden', 4),\n",
       "  ('gon', 23),\n",
       "  ('good', 65),\n",
       "  ('goodbye', 12),\n",
       "  ('gooooh', 1),\n",
       "  ('goplease', 1),\n",
       "  ('goso', 1),\n",
       "  ('got', 6),\n",
       "  ('gotfor', 1),\n",
       "  ('grab', 1),\n",
       "  ('grade', 1),\n",
       "  ('grant', 3),\n",
       "  ('gratitude', 1),\n",
       "  ('green', 2),\n",
       "  ('greet', 1),\n",
       "  ('grocery', 1),\n",
       "  ('groove', 2),\n",
       "  ('ground', 3),\n",
       "  ('groundif', 1),\n",
       "  ('group', 2),\n",
       "  ('grow', 21),\n",
       "  ('growin', 4),\n",
       "  ('gs', 1),\n",
       "  ('gstraight', 1),\n",
       "  ('gstre', 1),\n",
       "  ('guess', 14),\n",
       "  ('guide', 1),\n",
       "  ('guitar', 2),\n",
       "  ('gun', 3),\n",
       "  ('gut', 1),\n",
       "  ('guy', 5),\n",
       "  ('ha_ha', 1),\n",
       "  ('hailienowaday', 1),\n",
       "  ('hair', 12),\n",
       "  ('half', 3),\n",
       "  ('hand', 36),\n",
       "  ('handcuff', 1),\n",
       "  ('handed', 1),\n",
       "  ('handle', 14),\n",
       "  ('hang', 3),\n",
       "  ('hangin', 2),\n",
       "  ('hannibal', 1),\n",
       "  ('happen', 4),\n",
       "  ('happiness', 1),\n",
       "  ('happy', 9),\n",
       "  ('hard', 12),\n",
       "  ('harder', 1),\n",
       "  ('hate', 8),\n",
       "  ('hater', 1),\n",
       "  ('haunt', 1),\n",
       "  ('have', 93),\n",
       "  ('haveand', 1),\n",
       "  ('haveoops', 1),\n",
       "  ('head', 15),\n",
       "  ('heal', 1),\n",
       "  ('hear', 35),\n",
       "  ('heart', 60),\n",
       "  ('heartcause', 1),\n",
       "  ('hearti', 1),\n",
       "  ('heartlife', 1),\n",
       "  ('heartmost', 2),\n",
       "  ('heartnever', 1),\n",
       "  ('heartone', 1),\n",
       "  ('heartshow', 1),\n",
       "  ('heartthere', 1),\n",
       "  ('heartyour', 1),\n",
       "  ('heat', 1),\n",
       "  ('heaven', 3),\n",
       "  ('heavy', 1),\n",
       "  ('hell', 3),\n",
       "  ('hellbent', 1),\n",
       "  ('help', 13),\n",
       "  ('here', 3),\n",
       "  ('herebroadway', 2),\n",
       "  ('herei', 2),\n",
       "  ('hereoff', 1),\n",
       "  ('hereone', 1),\n",
       "  ('hero', 1),\n",
       "  ('hesitate', 1),\n",
       "  ('hide', 12),\n",
       "  ('high', 30),\n",
       "  ('high_heel', 1),\n",
       "  ('highly', 4),\n",
       "  ('highway', 1),\n",
       "  ('himlooke', 1),\n",
       "  ('hip', 1),\n",
       "  ('hit', 18),\n",
       "  ('hoe', 4),\n",
       "  ('hold', 24),\n",
       "  ('hole', 2),\n",
       "  ('home', 27),\n",
       "  ('homeive', 1),\n",
       "  ('homie', 2),\n",
       "  ('hoo', 2),\n",
       "  ('hood', 2),\n",
       "  ('hook', 2),\n",
       "  ('hop', 1),\n",
       "  ('hope', 22),\n",
       "  ('hopefully', 1),\n",
       "  ('hopelessly', 1),\n",
       "  ('horse', 1),\n",
       "  ('hot', 33),\n",
       "  ('hotcall', 1),\n",
       "  ('hotladie', 2),\n",
       "  ('hotlast', 1),\n",
       "  ('hour', 1),\n",
       "  ('house', 2),\n",
       "  ('how', 2),\n",
       "  ('howand', 1),\n",
       "  ('hug', 1),\n",
       "  ('hummersboy', 1),\n",
       "  ('humongous', 1),\n",
       "  ('hump', 2),\n",
       "  ('hunger', 2),\n",
       "  ('hurry', 1),\n",
       "  ('hurt', 11),\n",
       "  ('hurtin', 1),\n",
       "  ('hurting', 1),\n",
       "  ('hype', 1),\n",
       "  ('hypnotize', 1),\n",
       "  ('ice', 4),\n",
       "  ('idea', 7),\n",
       "  ('idiot', 1),\n",
       "  ('ignore', 2),\n",
       "  ('ii', 1),\n",
       "  ('ill', 74),\n",
       "  ('imagine', 3),\n",
       "  ('imitating', 8),\n",
       "  ('impress', 2),\n",
       "  ('inch', 1),\n",
       "  ('incomplete', 6),\n",
       "  ('incompleteeven', 1),\n",
       "  ('incredible', 1),\n",
       "  ('indecent', 1),\n",
       "  ('independent', 3),\n",
       "  ('infatuate', 1),\n",
       "  ('inner', 4),\n",
       "  ('innocent', 1),\n",
       "  ('innocentoop', 2),\n",
       "  ('innocentyeah', 1),\n",
       "  ('innocentyou', 1),\n",
       "  ('insanewhen', 1),\n",
       "  ('inshouldnt', 1),\n",
       "  ('inside', 14),\n",
       "  ('insidejust', 1),\n",
       "  ('insincere', 1),\n",
       "  ('inspire', 1),\n",
       "  ('instant', 1),\n",
       "  ('instantlywhere', 1),\n",
       "  ('instead', 2),\n",
       "  ('intercourse', 1),\n",
       "  ('interrupt', 1),\n",
       "  ('intoxication', 2),\n",
       "  ('introduction', 1),\n",
       "  ('intuition', 2),\n",
       "  ('invitation', 1),\n",
       "  ('ironic', 1),\n",
       "  ('ish', 3),\n",
       "  ('isoh', 1),\n",
       "  ('isone', 1),\n",
       "  ('issue', 2),\n",
       "  ('isthat', 1),\n",
       "  ('isyeah', 1),\n",
       "  ('itand', 1),\n",
       "  ('itdoesnt', 2),\n",
       "  ('itget', 1),\n",
       "  ('itor', 1),\n",
       "  ('itsometime', 1),\n",
       "  ('jade', 1),\n",
       "  ('jaguar', 4),\n",
       "  ('jail', 1),\n",
       "  ('jean', 3),\n",
       "  ('jeep', 1),\n",
       "  ('jerk', 1),\n",
       "  ('jewelry', 1),\n",
       "  ('job', 5),\n",
       "  ('joke', 2),\n",
       "  ('journey', 1),\n",
       "  ('joy', 5),\n",
       "  ('judgment', 2),\n",
       "  ('juice', 1),\n",
       "  ('jumpin', 2),\n",
       "  ('jumpin_jumpinladie', 6),\n",
       "  ('jumpinboy', 2),\n",
       "  ('junk', 1),\n",
       "  ('keep', 57),\n",
       "  ('key', 4),\n",
       "  ('kick', 9),\n",
       "  ('kicking', 1),\n",
       "  ('kid', 11),\n",
       "  ('kill', 3),\n",
       "  ('kind', 14),\n",
       "  ('kinda', 1),\n",
       "  ('king', 1),\n",
       "  ('kiss', 7),\n",
       "  ('knee', 2),\n",
       "  ('knickaboxer', 1),\n",
       "  ('knock', 3),\n",
       "  ('knockin', 1),\n",
       "  ('know', 302),\n",
       "  ('knowcause', 1),\n",
       "  ('knowif', 2),\n",
       "  ('knowledge', 1),\n",
       "  ('kryptonite', 2),\n",
       "  ('kryptoniteif', 1),\n",
       "  ('kryptoniteyou', 1),\n",
       "  ('lab', 1),\n",
       "  ('lace', 1),\n",
       "  ('lady', 21),\n",
       "  ('lance', 1),\n",
       "  ('land', 1),\n",
       "  ('landingwithout', 1),\n",
       "  ('last', 13),\n",
       "  ('late', 9),\n",
       "  ('lately', 1),\n",
       "  ('laugh', 3),\n",
       "  ('law', 2),\n",
       "  ('lead', 4),\n",
       "  ('lean', 1),\n",
       "  ('learn', 7),\n",
       "  ('least', 1),\n",
       "  ('leather', 2),\n",
       "  ('leave', 73),\n",
       "  ('leavequestion', 1),\n",
       "  ('lector', 1),\n",
       "  ('left', 6),\n",
       "  ('leg', 1),\n",
       "  ('lend', 1),\n",
       "  ('less', 1),\n",
       "  ('let', 166),\n",
       "  ('letter', 2),\n",
       "  ('lettin', 1),\n",
       "  ('lickylicky', 1),\n",
       "  ('lie', 12),\n",
       "  ('liebut', 1),\n",
       "  ('life', 86),\n",
       "  ('lifecause', 1),\n",
       "  ('lifeooh', 1),\n",
       "  ('lifepull', 1),\n",
       "  ('lifethere', 1),\n",
       "  ('lifetime', 5),\n",
       "  ('lifewaite', 1),\n",
       "  ('lift', 17),\n",
       "  ('light', 27),\n",
       "  ('like', 5),\n",
       "  ('line', 11),\n",
       "  ('lingerie', 1),\n",
       "  ('lip', 14),\n",
       "  ('list', 1),\n",
       "  ('listen', 21),\n",
       "  ('listenim', 2),\n",
       "  ('little', 31),\n",
       "  ('little_bit', 3),\n",
       "  ('live', 45),\n",
       "  ('lively', 4),\n",
       "  ('livinat', 1),\n",
       "  ('living', 1),\n",
       "  ('load', 2),\n",
       "  ('lock', 1),\n",
       "  ('loco', 1),\n",
       "  ('logic', 1),\n",
       "  ('loneliness', 2),\n",
       "  ('lonely', 14),\n",
       "  ('lonelyso', 1),\n",
       "  ('lonestar', 1),\n",
       "  ('long', 63),\n",
       "  ('longi', 1),\n",
       "  ('longing', 1),\n",
       "  ('longwaite', 3),\n",
       "  ('look', 63),\n",
       "  ('loose', 2),\n",
       "  ('loosen', 1),\n",
       "  ('lootin', 1),\n",
       "  ('lose', 37),\n",
       "  ('lot', 11),\n",
       "  ('loti', 1),\n",
       "  ('loud', 3),\n",
       "  ('loudhow', 1),\n",
       "  ('love', 357),\n",
       "  ('lovemost', 2),\n",
       "  ('lover', 4),\n",
       "  ...]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the corpus and dictionary, you need to provide the number of topics.\n",
    "\n",
    "- Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "- chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=50, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(49,\n",
      "  '0.000*\"be\" + 0.000*\"go\" + 0.000*\"get\" + 0.000*\"love\" + 0.000*\"let\" + '\n",
      "  '0.000*\"know\" + 0.000*\"baby\" + 0.000*\"make\" + 0.000*\"say\" + 0.000*\"s\"'),\n",
      " (9,\n",
      "  '0.001*\"be\" + 0.001*\"get\" + 0.001*\"know\" + 0.001*\"go\" + 0.000*\"love\" + '\n",
      "  '0.000*\"s\" + 0.000*\"want\" + 0.000*\"baby\" + 0.000*\"make\" + 0.000*\"girl\"'),\n",
      " (44,\n",
      "  '0.001*\"be\" + 0.001*\"love\" + 0.001*\"know\" + 0.001*\"get\" + 0.001*\"go\" + '\n",
      "  '0.000*\"say\" + 0.000*\"make\" + 0.000*\"s\" + 0.000*\"want\" + 0.000*\"can\"'),\n",
      " (15,\n",
      "  '0.001*\"be\" + 0.001*\"know\" + 0.001*\"get\" + 0.001*\"go\" + 0.001*\"love\" + '\n",
      "  '0.001*\"make\" + 0.001*\"baby\" + 0.001*\"s\" + 0.001*\"let\" + 0.000*\"say\"'),\n",
      " (43,\n",
      "  '0.001*\"be\" + 0.001*\"get\" + 0.001*\"go\" + 0.001*\"know\" + 0.001*\"love\" + '\n",
      "  '0.001*\"make\" + 0.001*\"want\" + 0.000*\"girl\" + 0.000*\"say\" + 0.000*\"baby\"'),\n",
      " (2,\n",
      "  '0.002*\"be\" + 0.001*\"go\" + 0.001*\"get\" + 0.001*\"know\" + 0.001*\"baby\" + '\n",
      "  '0.001*\"love\" + 0.001*\"s\" + 0.001*\"make\" + 0.001*\"say\" + 0.001*\"see\"'),\n",
      " (12,\n",
      "  '0.002*\"be\" + 0.002*\"get\" + 0.001*\"go\" + 0.001*\"love\" + 0.001*\"baby\" + '\n",
      "  '0.001*\"know\" + 0.001*\"make\" + 0.001*\"girl\" + 0.001*\"s\" + 0.001*\"say\"'),\n",
      " (21,\n",
      "  '0.003*\"be\" + 0.003*\"get\" + 0.002*\"love\" + 0.002*\"go\" + 0.002*\"know\" + '\n",
      "  '0.002*\"make\" + 0.001*\"let\" + 0.001*\"baby\" + 0.001*\"s\" + 0.001*\"say\"'),\n",
      " (26,\n",
      "  '0.003*\"be\" + 0.002*\"go\" + 0.002*\"get\" + 0.001*\"love\" + 0.001*\"know\" + '\n",
      "  '0.001*\"make\" + 0.001*\"want\" + 0.001*\"girl\" + 0.001*\"s\" + 0.001*\"see\"'),\n",
      " (28,\n",
      "  '0.003*\"be\" + 0.002*\"get\" + 0.002*\"go\" + 0.002*\"know\" + 0.002*\"make\" + '\n",
      "  '0.001*\"love\" + 0.001*\"say\" + 0.001*\"s\" + 0.001*\"come\" + 0.001*\"girl\"'),\n",
      " (6,\n",
      "  '0.012*\"be\" + 0.006*\"get\" + 0.005*\"know\" + 0.004*\"go\" + 0.004*\"love\" + '\n",
      "  '0.003*\"want\" + 0.003*\"make\" + 0.003*\"let\" + 0.003*\"girl\" + 0.002*\"say\"'),\n",
      " (29,\n",
      "  '0.010*\"be\" + 0.006*\"get\" + 0.006*\"go\" + 0.005*\"know\" + 0.004*\"love\" + '\n",
      "  '0.003*\"let\" + 0.003*\"s\" + 0.003*\"girl\" + 0.003*\"baby\" + 0.003*\"want\"'),\n",
      " (8,\n",
      "  '0.012*\"be\" + 0.007*\"go\" + 0.005*\"love\" + 0.004*\"know\" + 0.004*\"get\" + '\n",
      "  '0.003*\"let\" + 0.003*\"s\" + 0.003*\"take\" + 0.003*\"want\" + 0.002*\"baby\"'),\n",
      " (46,\n",
      "  '0.011*\"be\" + 0.007*\"get\" + 0.006*\"go\" + 0.004*\"know\" + 0.003*\"say\" + '\n",
      "  '0.003*\"love\" + 0.003*\"make\" + 0.003*\"let\" + 0.003*\"baby\" + 0.003*\"s\"'),\n",
      " (13,\n",
      "  '0.015*\"be\" + 0.006*\"go\" + 0.005*\"get\" + 0.004*\"love\" + 0.004*\"know\" + '\n",
      "  '0.004*\"want\" + 0.003*\"baby\" + 0.003*\"let\" + 0.003*\"say\" + 0.003*\"girl\"'),\n",
      " (17,\n",
      "  '0.015*\"be\" + 0.007*\"go\" + 0.006*\"know\" + 0.006*\"get\" + 0.004*\"say\" + '\n",
      "  '0.004*\"love\" + 0.004*\"make\" + 0.004*\"s\" + 0.004*\"see\" + 0.003*\"let\"'),\n",
      " (38,\n",
      "  '0.053*\"be\" + 0.027*\"go\" + 0.025*\"get\" + 0.017*\"love\" + 0.016*\"know\" + '\n",
      "  '0.013*\"let\" + 0.013*\"tell\" + 0.013*\"want\" + 0.012*\"feel\" + 0.011*\"say\"'),\n",
      " (42,\n",
      "  '0.048*\"be\" + 0.022*\"love\" + 0.020*\"get\" + 0.020*\"go\" + 0.019*\"let\" + '\n",
      "  '0.017*\"never\" + 0.016*\"know\" + 0.014*\"baby\" + 0.012*\"make\" + 0.012*\"come\"'),\n",
      " (22,\n",
      "  '0.051*\"be\" + 0.028*\"get\" + 0.025*\"go\" + 0.022*\"love\" + 0.016*\"know\" + '\n",
      "  '0.012*\"baby\" + 0.011*\"make\" + 0.010*\"let\" + 0.010*\"want\" + 0.010*\"come\"'),\n",
      " (11,\n",
      "  '0.050*\"be\" + 0.028*\"get\" + 0.025*\"go\" + 0.022*\"know\" + 0.017*\"love\" + '\n",
      "  '0.014*\"make\" + 0.013*\"say\" + 0.013*\"s\" + 0.012*\"baby\" + 0.012*\"girl\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Model Perplexity and Coherence Score\n",
    "- Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. \n",
    "- a quick explanation: https://rare-technologies.com/what-is-topic-coherence/\n",
    "- a paper: http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.738050055584585\n",
      "\n",
      "Coherence Score:  0.288282574524703\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f32040dc6375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize the topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mmds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    396\u001b[0m    \u001b[0mterm_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m    \u001b[0mtopic_info\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m    \u001b[0mtoken_table\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m    \u001b[0mtopic_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m    top_terms = pd.concat(Parallel(n_jobs=n_jobs)(delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls) \\\n\u001b[0;32m--> 255\u001b[0;31m                                                  for ls in _job_chunks(lambda_seq, n_jobs)))\n\u001b[0m\u001b[1;32m    256\u001b[0m    \u001b[0mtopic_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_top_term_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_term_info\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    552\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word,mds='mmds')\n",
    "vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "- A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "- A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "- if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic.\n",
    "\n",
    "- Given our prior knowledge of the number of natural topics in the document, finding the best model was fairly straightforward.\n",
    "\n",
    "Upnext, we will improve upon this model by using Mallet’s version of LDA algorithm and then we will focus on how to arrive at the optimal number of topics given any large corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mallet’s version, however, often gives a better quality of topics.\n",
    "\n",
    "Gensim provides a wrapper to implement Mallet’s LDA from within Gensim itself. You only need to download the zipfile, unzip it and provide the path to mallet in the unzipped directory to gensim.models.wrappers.LdaMallet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8ed8ad695f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmallet_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/hu/Desktop/BPO2019fall/TM3/mallet-2.0.8/bin/mallet'\u001b[0m\u001b[0;31m# update this path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mldamallet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmallet_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# NOTE \"--keep-sequence-bigrams\" / \"--use-ngrams true\" poorer results + runs out of memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training MALLET LDA with %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# NOTE - we are still keeping the wordtopics variable to not break backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"COMMAND: %s %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    924\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Building LDA Mallet Model\n",
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "mallet_path = '/Users/hu/Desktop/BPO2019fall/TM3/mallet-2.0.8/bin/mallet'# update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, here afterchanging the LDA algorithm, the coherence score dropeed from .48 to .38."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal number of topics for LDA?\n",
    "- build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.\n",
    "\n",
    "- Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.\n",
    "\n",
    "- If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large.\n",
    "\n",
    "- The compute_coherence_values() (see below) trains multiple LDA models and provides the models and their corresponding coherence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the coherence score seems to keep increasing, it may make better sense to pick the model that gave the highest CV before flattening out. This is exactly the case here.\n",
    "\n",
    "So for further steps, here we choose the model with 20 topics (exactlt what we have gotten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics = 2  has Coherence Value of 0.3008\n",
      "Num Topics = 8  has Coherence Value of 0.352\n",
      "Num Topics = 14  has Coherence Value of 0.3511\n",
      "Num Topics = 20  has Coherence Value of 0.38\n",
      "Num Topics = 26  has Coherence Value of 0.3794\n",
      "Num Topics = 32  has Coherence Value of 0.3758\n",
      "Num Topics = 38  has Coherence Value of 0.3723\n"
     ]
    }
   ],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the topics for the chosen LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.038*\"history\" + 0.035*\"great\" + 0.017*\"period\" + 0.015*\"present\" + '\n",
      "  '0.014*\"early\" + 0.013*\"volume\" + 0.011*\"give\" + 0.011*\"time\" + '\n",
      "  '0.011*\"historical\" + 0.011*\"page\"'),\n",
      " (1,\n",
      "  '0.035*\"thing\" + 0.025*\"good\" + 0.023*\"man\" + 0.022*\"turn\" + 0.020*\"young\" + '\n",
      "  '0.017*\"people\" + 0.013*\"talk\" + 0.011*\"call\" + 0.011*\"bad\" + '\n",
      "  '0.010*\"moment\"'),\n",
      " (2,\n",
      "  '0.028*\"war\" + 0.023*\"make\" + 0.017*\"great\" + 0.014*\"time\" + 0.014*\"german\" '\n",
      "  '+ 0.012*\"year\" + 0.009*\"man\" + 0.009*\"leave\" + 0.008*\"peace\" + '\n",
      "  '0.008*\"fight\"'),\n",
      " (3,\n",
      "  '0.027*\"book\" + 0.020*\"interesting\" + 0.020*\"make\" + 0.019*\"chapter\" + '\n",
      "  '0.019*\"case\" + 0.017*\"school\" + 0.016*\"give\" + 0.016*\"fact\" + 0.013*\"study\" '\n",
      "  '+ 0.010*\"bring\"'),\n",
      " (4,\n",
      "  '0.020*\"man\" + 0.015*\"make\" + 0.014*\"find\" + 0.013*\"time\" + 0.013*\"day\" + '\n",
      "  '0.010*\"fall\" + 0.008*\"eye\" + 0.008*\"dead\" + 0.007*\"night\" + 0.007*\"water\"'),\n",
      " (5,\n",
      "  '0.097*\"book\" + 0.053*\"write\" + 0.038*\"read\" + 0.034*\"make\" + 0.034*\"author\" '\n",
      "  '+ 0.028*\"good\" + 0.020*\"life\" + 0.019*\"writer\" + 0.018*\"man\" + '\n",
      "  '0.015*\"reader\"'),\n",
      " (6,\n",
      "  '0.054*\"man\" + 0.027*\"life\" + 0.019*\"world\" + 0.016*\"human\" + '\n",
      "  '0.010*\"religion\" + 0.010*\"truth\" + 0.009*\"mind\" + 0.009*\"modern\" + '\n",
      "  '0.009*\"true\" + 0.009*\"find\"'),\n",
      " (7,\n",
      "  '0.032*\"life\" + 0.026*\"love\" + 0.017*\"heart\" + 0.013*\"feel\" + 0.012*\"sense\" '\n",
      "  '+ 0.012*\"beauty\" + 0.011*\"live\" + 0.011*\"death\" + 0.011*\"nature\" + '\n",
      "  '0.009*\"beautiful\"'),\n",
      " (8,\n",
      "  '0.044*\"child\" + 0.039*\"book\" + 0.031*\"picture\" + 0.020*\"colour\" + '\n",
      "  '0.018*\"illustration\" + 0.015*\"find\" + 0.015*\"illustrate\" + 0.014*\"draw\" + '\n",
      "  '0.013*\"delightful\" + 0.012*\"boy\"'),\n",
      " (9,\n",
      "  '0.030*\"book\" + 0.029*\"volume\" + 0.020*\"author\" + 0.019*\"publish\" + '\n",
      "  '0.017*\"work\" + 0.016*\"year\" + 0.013*\"number\" + 0.012*\"publisher\" + '\n",
      "  '0.012*\"print\" + 0.010*\"note\"'),\n",
      " (10,\n",
      "  '0.017*\"end\" + 0.014*\"mystery\" + 0.010*\"murder\" + 0.010*\"part\" + '\n",
      "  '0.010*\"real\" + 0.010*\"lie\" + 0.009*\"follow\" + 0.009*\"late\" + 0.009*\"make\" + '\n",
      "  '0.009*\"place\"'),\n",
      " (11,\n",
      "  '0.031*\"poet\" + 0.024*\"poem\" + 0.024*\"poetry\" + 0.019*\"music\" + '\n",
      "  '0.017*\"write\" + 0.016*\"word\" + 0.016*\"verse\" + 0.013*\"work\" + 0.013*\"line\" '\n",
      "  '+ 0.010*\"beauty\"'),\n",
      " (12,\n",
      "  '0.058*\"woman\" + 0.037*\"life\" + 0.031*\"love\" + 0.029*\"young\" + '\n",
      "  '0.026*\"family\" + 0.021*\"wife\" + 0.018*\"mother\" + 0.017*\"girl\" + '\n",
      "  '0.016*\"character\" + 0.015*\"daughter\"'),\n",
      " (13,\n",
      "  '0.043*\"write\" + 0.029*\"letter\" + 0.027*\"friend\" + 0.021*\"time\" + '\n",
      "  '0.015*\"great\" + 0.014*\"long\" + 0.011*\"die\" + 0.011*\"day\" + 0.009*\"year\" + '\n",
      "  '0.009*\"page\"'),\n",
      " (14,\n",
      "  '0.021*\"country\" + 0.018*\"give\" + 0.014*\"work\" + 0.014*\"interest\" + '\n",
      "  '0.013*\"describe\" + 0.013*\"picture\" + 0.012*\"good\" + 0.011*\"long\" + '\n",
      "  '0.011*\"town\" + 0.010*\"place\"'),\n",
      " (15,\n",
      "  '0.055*\"play\" + 0.021*\"make\" + 0.017*\"stage\" + 0.016*\"good\" + 0.015*\"give\" + '\n",
      "  '0.014*\"part\" + 0.012*\"scene\" + 0.012*\"drama\" + 0.011*\"character\" + '\n",
      "  '0.011*\"act\"'),\n",
      " (16,\n",
      "  '0.141*\"story\" + 0.024*\"character\" + 0.022*\"reader\" + 0.020*\"tale\" + '\n",
      "  '0.020*\"adventure\" + 0.018*\"author\" + 0.014*\"romance\" + 0.013*\"short\" + '\n",
      "  '0.012*\"girl\" + 0.012*\"plot\"'),\n",
      " (17,\n",
      "  '0.012*\"mind\" + 0.012*\"idea\" + 0.011*\"fact\" + 0.010*\"form\" + 0.009*\"nature\" '\n",
      "  '+ 0.008*\"sense\" + 0.008*\"reason\" + 0.008*\"method\" + 0.008*\"experience\" + '\n",
      "  '0.008*\"question\"'),\n",
      " (18,\n",
      "  '0.038*\"art\" + 0.037*\"work\" + 0.023*\"great\" + 0.019*\"literature\" + '\n",
      "  '0.015*\"literary\" + 0.015*\"writer\" + 0.014*\"critic\" + 0.014*\"french\" + '\n",
      "  '0.012*\"criticism\" + 0.010*\"time\"'),\n",
      " (19,\n",
      "  '0.013*\"people\" + 0.013*\"country\" + 0.013*\"political\" + 0.009*\"government\" + '\n",
      "  '0.008*\"nation\" + 0.008*\"society\" + 0.008*\"state\" + 0.008*\"today\" + '\n",
      "  '0.007*\"power\" + 0.007*\"british\"')]\n"
     ]
    }
   ],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the dominant topic in each sentence\n",
    "- One of the practical application of topic modeling is to determine what topic a given document is about.\n",
    "\n",
    "- To find that, we find the topic number that has the highest percentage contribution in that document.\n",
    "\n",
    "- The format_topics_sentences() function below nicely aggregates this information in a presentable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>history, great, period, present, early, volume...</td>\n",
       "      <td>stonge htrory lord george gordon percy colmon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>write, letter, friend, time, great, long, die,...</td>\n",
       "      <td>shakespeare seventeenth century manuscript e m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.2846</td>\n",
       "      <td>end, mystery, murder, part, real, lie, follow,...</td>\n",
       "      <td>shakespeare sonnet edward de vere gerald h ren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>poet, poem, poetry, music, write, word, verse,...</td>\n",
       "      <td>shakespeare music denit pworoemsh tmwx sout k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>book, interesting, make, chapter, case, school...</td>\n",
       "      <td>waiting city abridg ment louis sebastien merci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>play, make, stage, good, give, part, scene, dr...</td>\n",
       "      <td>new edition twelfth night edited sir arthur qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>write, letter, friend, time, great, long, die,...</td>\n",
       "      <td>parable lover lewis gibbs 7 6d dent we heard l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>country, give, work, interest, describe, pictu...</td>\n",
       "      <td>dzoa lovzse john c moore dent 75 6d mr moore f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3510</td>\n",
       "      <td>war, make, great, time, german, year, man, lea...</td>\n",
       "      <td>air raid war air vol iii h a jones 7 6d milfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>book, volume, author, publish, work, year, num...</td>\n",
       "      <td>first lord cowley wellington brother diary cor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             0.0              0.1056   \n",
       "1            1            13.0              0.2698   \n",
       "2            2            10.0              0.2846   \n",
       "3            3            11.0              0.1355   \n",
       "4            4             3.0              0.1185   \n",
       "5            5            15.0              0.1453   \n",
       "6            6            13.0              0.1214   \n",
       "7            7            14.0              0.1375   \n",
       "8            8             2.0              0.3510   \n",
       "9            9             9.0              0.1347   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  history, great, period, present, early, volume...   \n",
       "1  write, letter, friend, time, great, long, die,...   \n",
       "2  end, mystery, murder, part, real, lie, follow,...   \n",
       "3  poet, poem, poetry, music, write, word, verse,...   \n",
       "4  book, interesting, make, chapter, case, school...   \n",
       "5  play, make, stage, good, give, part, scene, dr...   \n",
       "6  write, letter, friend, time, great, long, die,...   \n",
       "7  country, give, work, interest, describe, pictu...   \n",
       "8  war, make, great, time, german, year, man, lea...   \n",
       "9  book, volume, author, publish, work, year, num...   \n",
       "\n",
       "                                                Text  \n",
       "0  stonge htrory lord george gordon percy colmon ...  \n",
       "1  shakespeare seventeenth century manuscript e m...  \n",
       "2  shakespeare sonnet edward de vere gerald h ren...  \n",
       "3   shakespeare music denit pworoemsh tmwx sout k...  \n",
       "4  waiting city abridg ment louis sebastien merci...  \n",
       "5  new edition twelfth night edited sir arthur qu...  \n",
       "6  parable lover lewis gibbs 7 6d dent we heard l...  \n",
       "7  dzoa lovzse john c moore dent 75 6d mr moore f...  \n",
       "8  air raid war air vol iii h a jones 7 6d milfor...  \n",
       "9  first lord cowley wellington brother diary cor...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most representative document for each topic\n",
    "- Sometimes just the topic keywords may not be enough to make sense of what a topic is about. \n",
    "- To help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2749</td>\n",
       "      <td>history, great, period, present, early, volume...</td>\n",
       "      <td>annals south africa eistory south africa 18731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2883</td>\n",
       "      <td>thing, good, man, turn, young, people, talk, c...</td>\n",
       "      <td>immortal jane mary webb mr h b l webb must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4725</td>\n",
       "      <td>war, make, great, time, german, year, man, lea...</td>\n",
       "      <td>war airand butcher shop save fact common denom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6302</td>\n",
       "      <td>book, interesting, make, chapter, case, school...</td>\n",
       "      <td>inner life pauper asylum dr montagu lomax engl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5418</td>\n",
       "      <td>man, make, find, time, day, fall, eye, dead, n...</td>\n",
       "      <td>tree god t f powys mr told great man farmed l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0              0.2749   \n",
       "1        1.0              0.2883   \n",
       "2        2.0              0.4725   \n",
       "3        3.0              0.6302   \n",
       "4        4.0              0.5418   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  history, great, period, present, early, volume...   \n",
       "1  thing, good, man, turn, young, people, talk, c...   \n",
       "2  war, make, great, time, german, year, man, lea...   \n",
       "3  book, interesting, make, chapter, case, school...   \n",
       "4  man, make, find, time, day, fall, eye, dead, n...   \n",
       "\n",
       "                                                Text  \n",
       "0  annals south africa eistory south africa 18731...  \n",
       "1  immortal jane mary webb mr h b l webb must be ...  \n",
       "2  war airand butcher shop save fact common denom...  \n",
       "3  inner life pauper asylum dr montagu lomax engl...  \n",
       "4   tree god t f powys mr told great man farmed l...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the most representative document for each topic\n",
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic distribution across documents\n",
    "- To understand the volume and distribution of topics in order to judge how widely it was discussed. \n",
    "- The below table exposes that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>history, great, period, present, early, volume...</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.0510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>write, letter, friend, time, great, long, die,...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>end, mystery, murder, part, real, lie, follow,...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.0442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>poet, poem, poetry, music, write, word, verse,...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>book, interesting, make, chapter, case, school...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.0442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>write, letter, friend, time, great, long, die,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2801.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>child, book, picture, colour, illustration, fi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2802.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>woman, life, love, young, family, wife, mother...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2803.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>man, life, world, human, religion, truth, mind...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2804.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>child, book, picture, colour, illustration, fi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2805 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic                                     Topic_Keywords  \\\n",
       "0.0                0.0  history, great, period, present, early, volume...   \n",
       "1.0               13.0  write, letter, friend, time, great, long, die,...   \n",
       "2.0               10.0  end, mystery, murder, part, real, lie, follow,...   \n",
       "3.0               11.0  poet, poem, poetry, music, write, word, verse,...   \n",
       "4.0                3.0  book, interesting, make, chapter, case, school...   \n",
       "...                ...                                                ...   \n",
       "2800.0            13.0  write, letter, friend, time, great, long, die,...   \n",
       "2801.0             8.0  child, book, picture, colour, illustration, fi...   \n",
       "2802.0            12.0  woman, life, love, young, family, wife, mother...   \n",
       "2803.0             6.0  man, life, world, human, religion, truth, mind...   \n",
       "2804.0             8.0  child, book, picture, colour, illustration, fi...   \n",
       "\n",
       "        Num_Documents  Perc_Documents  \n",
       "0.0             143.0          0.0510  \n",
       "1.0             100.0          0.0357  \n",
       "2.0             124.0          0.0442  \n",
       "3.0              73.0          0.0260  \n",
       "4.0             124.0          0.0442  \n",
       "...               ...             ...  \n",
       "2800.0            NaN             NaN  \n",
       "2801.0            NaN             NaN  \n",
       "2802.0            NaN             NaN  \n",
       "2803.0            NaN             NaN  \n",
       "2804.0            NaN             NaN  \n",
       "\n",
       "[2805 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Topic distribution across documents\n",
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
